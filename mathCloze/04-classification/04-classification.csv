#separator:|
#html:true
itsl-401|Generative models for classification|"Suppose \(Y\) can take on \(K\) distinct class values. Let \(\pi _k\) represent the overall (prior) probability that a randomly chosen observation comes from the \(k^\text {th}\) class. Let \(f_k(X) := \text{Pr}(X | Y = k)\) denote the density function of \(X\) for an observation that comes from the \(k^\text {th}\) class. Then, the Bayes’ theorem states {{c1:: <div class=""displaymath"" id=""a0000000002"">  \[  p_k(x) = \mathsf{Pr}(Y = k | X = x) = \frac{\pi _k f_k(x)}{\sum _{l = 1}^K \pi _l f_l(x)}.  \]</div> }}"||Classification||
itsl-402|Linear discrimination analysis for \(p = 1\)|"It is assumed that \(f_k(x) := \mathsf{Pr}(X = x| Y = k)\) is normal. In the one-dimensional setting, the normal density takes the form {{c1:: <div class=""displaymath"" id=""a0000000003"">  \[  f_k(x) = \frac{1}{\sqrt{2\pi \sigma _k^2} } \exp \Bigl(-\frac{1}{2\sigma _k^2} (x - \mu _k)^2 \Bigr),  \]</div> }} where \(\mu _k\) and \(\sigma _k^2\) are the {{c1::mean and variance parameters for the \(k^\text {th}\) class.}}"||Classification||
itsl-403|Linear discrimination analysis for \(p = 1\)|"The Bayes Classifier involves {{c1::assigning an observation \(X = x\) to the class for which \(p_k(x)\) is largest}}. Assuming \(f_k(x)\) is normal and that there is a shared variance term across all \(K\) classes, this is equivalent to assigning the observation to the class for which {{c2:: <div class=""displaymath"" id=""a0000000004"">  \[  \delta _k(x) = x \cdot \frac{\mu _k}{\sigma ^2} - \frac{\mu _k^2}{\sigma ^2} + \log \pi _k,  \]</div> }} is largest."||Classification
itsl-404|Linear discrimination analysis for \(p = 1\)|"The linear discriminant analysis (LDA) method approximates the Bayes classifier by using estimates for \(\hat\pi _k, \,  \hat\mu _k\) and \(\hat\sigma ^2\), given by,<br/>{{c1:: <div class=""displaymath"" id=""a0000000005"">  \[  \begin{aligned}  \hat\mu _k &amp; = \frac{1}{n_k} \sum _{i:y_i = k} x_i\\ \hat\sigma ^2 &amp; = \frac{1}{n - K} \sum _{k = 1}^K \sum _{i:y_i = k} (x_i - \hat\mu _k)^2\\ \hat\pi _k &amp; = \frac{n_k}{n}, \end{aligned}  \]</div> }} <br/>where \(n\) is the total number of training observations and \(n_k\) is the number in the \(k^\text {th}\) class."||Classification
itsl-405|Linear discrimination analysis for \(p = 1\)|The LDA classifier results from assuming that the observations {{c1::within each class come from a normal distribution}} with a {{c2::class specific mean and common variance \(\sigma ^2\)}}, and plugging esimates for those parameters into the {{c3::Bayes classifier}}.||Classification
itsl-406|Classification and diagnostic testing|"<table class=""tabular""><tr><td colspan="""" rowspan="""" style=""text-align:left""><p> False positive rate </p></td><td colspan="""" rowspan="""" style=""text-align:left""><p> {{c1:: FP / N }}</p></td><td colspan="""" rowspan="""" style=""text-align:left""><p> {{c1:: Type I error, 1 - Specificity }}</p></td></tr><tr><td colspan="""" rowspan="""" style=""text-align:left""><p>True positive rate </p></td><td colspan="""" rowspan="""" style=""text-align:left""><p> {{c1:: TP / P }}</p></td><td colspan="""" rowspan="""" style=""text-align:left""><p> {{c1:: 1 - Type II Error, power, sensitivity, recall }}</p></td></tr><tr><td colspan="""" rowspan="""" style=""text-align:left""><p>Positive predictive value </p></td><td colspan="""" rowspan="""" style=""text-align:left""><p> {{c1:: TP / \(\hat{\text{P} }\) }}</p></td><td colspan="""" rowspan="""" style=""text-align:left""><p> {{c1:: Precision, 1 - false discovery proportion }}</p></td></tr><tr><td colspan="""" rowspan="""" style=""text-align:left""><p>Negative predictive value </p></td><td colspan="""" rowspan="""" style=""text-align:left""><p> {{c1:: TN / \(\hat{\text{N} }\) }}</p></td><td colspan="""" rowspan="""" style=""text-align:left""> </td></tr></table>"||Classification
itsl-407|Quadratic discriminant analysis|What is the difference between QDA and LDA? <p>QDA assumes that an observation from the \(k^\text {th}\) class is of the form {{c1::\(X \sim \mathcal{N}(\mu _k, \Sigma _k)\)}}, where {{c1::\(\Sigma _k\) is a covariance matrix for the \(k^\text {th}\) class}}. LDA assumes that all observations {{c1::share a common covariance matrix \(\Sigma \)}}.  </p>||Classification||
itsl-408|Naive Bayes|"The naive Bayes classifier makes the assumption that, {{c1:: <div class=""centered""><p>Within the \(k^\text {th}\) class, the \(p\) predictors are independent. </p></div> }} Mathematically, this assumption means, {{c2:: <div class=""displaymath"" id=""a0000000006"">  \[  f_k(x) = f_{k1}(x_1) \times f_{k2} x_2 \times \cdots \times f_{kp}(x_p),  \]</div> }} where {{c2::\(f_{kj}\) is the density function of the \(j^\text {th}\) predictor among observations in the \(k^\text {th}\) class}}."||Classification||
itsl-409|Naive Bayes|"With the naive Bayes assumption (that the \(p\) covariates are independent within each class), and using Bayes theorem, we get posterior probability, {{c1:: <div class=""displaymath"" id=""a0000000007"">  \[  \mathsf{Pr}(Y = k | X = x) = \frac{\pi _k \times f_{k1}(x_1) \times \cdots \times f_{kp}(x_p) }{\sum _{l = 1}^K \pi _l \times f_{l1}(x_1) \times \cdots \times f_{lp}(x_p)},  \]</div> }} for \(k = 1, \ldots , K\)."||Classification
itsl-410|Naive Bayes|"Three options to estimate the one-dimensional density function \(f_{kj}\) using training data \(x_{1j}, \ldots , x_{pj}\) are, <div class=""centered""> <ul class=""itemize""><li><p>\(X_j\) quantitative: {{c1:: </p><ul class=""itemize""><li><p>Can assume \(X_j | Y \sim \mathcal{N}(\mu _{jk}, \sigma _{jk}^2)\) (this is QDA with an additional assumption that the class-specific covariance matrix is diagonal); </p></li><li><p>Non-parametric estimate using the histogram or kernel density function. </p></li></ul><p> }} </p></li><li><p>\(X_j\) qualitative: {{c2::count the proportion of training observations for the \(j^\text {th}\) predictor corresponding to each class}}. </p></li></ul> </div>"||Classification
itsl-411|KNN vs LDA and QDA|"<div class=""centered""> <ul class=""itemize""><li><p>KNN is {{c1::non-parametric}} and therefore we expect it to dominate LDA and logistic regression when {{c1::the decision boundary is highly non-linear, provided \(n\) is large and \(p\) is small}}. </p></li><li><p>Where the decision boundary is non-lienar but \(n\) is modest or \(p\) is not very small, then {{c2::QDA may be preferred to KNN}}. This is because {{c2::QDA can prove a non-linear decision boundary while taking advantage of a parametric form}}. </p></li><li><p>Unlike logistic regression, KNN does not {{c3::tell us which predictors are important}}. </p></li></ul> </div>"||Classification
