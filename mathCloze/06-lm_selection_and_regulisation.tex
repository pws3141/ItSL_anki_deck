\documentclass[10pt]{article}

\usepackage{MathCloze}

% some user macros:
\newcommand{\CC}{\mathbb{C}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\PP}{\mathsf{Pr}}

% use \def instead of \renewcommand
\def\phi{\varphi}

% DeclareMathOperator should also work
\DeclareMathOperator{\Mat}{Mat}
   
\begin{document}

% field order
% 1. Title
% 2. Front side content
% 3. Back side content
% 4. Topic
% 5. Type
% 6. Note number

\begin{note}{itsl-601}
  \field\ Best Subset Selection
  \field\ 
        To perform best subset selection, we fit \cloze{1}a seperate least squares
        regression for each possible combination of the $p$ predictors\clend{}.

        The procedure is (where $\mathcal{M}_0$ is the null model with no predictors):
        \begin{enumerate}
                \item For $k = 1, \ldots, p$:
                \cloze{2}
                \begin{enumerate}
                        \item Fit all ${p \choose k}$ models that contain exactly
                                $k$ predictors;
                        \item Pick the ``best'' model, $\mathcal{M}_k$, such that
                                \textit{e.g.}\ RSS is minimised, or $R^2$ maximised.
                \end{enumerate}
                \clend{}
        \item \cloze{2}Select the single best model from $\mathcal{M}_0, \ldots,
                \mathcal{M}_p$,
                using \textit{e.g.}\ the prediction error on a validation set,
                        adjusted $R^2$, or cross validation.\clend{}
        \end{enumerate}
        
  \field\ %back
  \field\ Linear Model Selection %topic
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-602}
  \field\ Best Subset Selection
        \field\ Best subset selection suffers from \cloze{1}computational
        limitations\clend{}. In general, there are \cloze{1}$2^p$ models that
        involve subsets of $p$ predictors\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-603}
  \field\ Forward Stepwise Selection
        \field\ Forward stepwise selection begins with \cloze{1}a model containing
        no predictors, and then adds predictors to the model, one-at-a-time, until
        all of the predictors are in the model\clend{}. At each step \cloze{1}the
        variable that gives the greatest additional improvement to the fit is added
        to the model\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-604}
  \field\ Forward Stepwise Selection
\field\ Forward stepwise selection involves fitting a total of
        \cloze{1}$\sum_{i = 0}^{p - 1} (p - k) = 1 + \frac{p(p + 1)}{2}$\clend{}
        models.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-605}
  \field\ Forward Stepwise Selection
        \field\ Forward stepwise selection can be applied even in the scenario
        \cloze{1}$n < p$,
        although here only the first $n$ stepwise models, $\mathcal{M}_0, \ldots,
        \mathcal{M}_{n - 1}$, can be found\clend{}. Beyond this, \cloze{1}least
        squares does not give a unique solution\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-606}
  \field\ Adjusting the training error
  \field\ Four approaches to selecting among a set of models with different number
        of variables are: \cloze{1}$C_p$, Akaike information criterion (AIC), Bayesian
        information criterion (BIC), and adjusted $R^2$\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-607}
  \field\ $C_p$ estimate of test MSE
  \field\ For a fitted least squares model containing $d$ predictors, the $C_p$
        estimate of test MSE is given by,
        \cloze{1}
        \[
        C_p = \frac{1}{n} (RSS + 2d \hat{\sigma}^2),
        \]
        \clend{}
        where \cloze{1}$\hat{\sigma}^2$ is an estimate of the variance of the error
        $\varepsilon$ associated with each response measurement\clend{}.
        Typically, \cloze{2}$\hat{\sigma}^2$ is estimated using the full model
        containing all predictors\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-608}
  \field\ $C_p$ estimate of test MSE
        \field\ Essentially, the $C_p$ statistic \cloze{1}adds a penalty of
        $2d\hat{\sigma}^2$ to the training RSS in order to adjust for the fact that
        the training error tends to underestimate the test error\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-609}
  \field\ Akaike Information Criterion
\field\ For a standard multiple regression model, least squares and
        maximum likelihood are the same. In this case AIC is given by
        \cloze{1}
        \[
        \text{AIC} \propto \frac{1}{n} (RSS + 2d \hat{\sigma}^2).
        \]
        \clend\
        Hence, \cloze{1} for least squares models, $C_p$ and AIC are proportional to
        each other\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-610}
  \field\ Bayesian Information Criterion
  \field\ For a least squares model with $d$ predictors, the BIC is given by
        \cloze{1}
        \[
        \text{BIC} \propto \frac{1}{n} (\text{RSS} + \log(n) d \hat{\sigma}^2).
        \]
        \clend\
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-611}
  \field\ Adjusted-$R^2$
  \field\ For a least squares model with $d$ predictors, the adjusted $R^2$
        statistic is given by
        \cloze{1}
        \[
                \text{Adjusted } R^2 = 1 - \frac{\text{RSS} / (n - d- 1)}{\text{TSS}
                / (n - 1)}.
        \]
        \clend\
        A \cloze{2} large value \clend\ of adjusted $R^2$ indicates a model with
        \cloze{2} small test error \clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}


\begin{note}{itsl-612}
  \field\ Adjusted-$R^2$
  \field\ The intuition behind the adjusted $R^2$ is that once all of the correct
        variables have been included in the model, adding \cloze{1}additional noise
        variables will lead to only a very small decrease in RSS\clend{}.  Since
        adding \cloze{1}noise variables leads to an increase in $d$, such variables
        will lead to an increase in $RSS/(n - d - 1)$, and consequently a decrease
        in the adjusted $R^2$\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-613}
  \field\ Validation and cross-validation
  \field\ Using validation and cross-validation procedures to estimate test error
        has an advantage relative to AIC, BIC, $C_p$, and adjusted $R^2$, in that
        \cloze{1}it provides a direct estimate of the test error, and makes fewer
        assumptions about the true underlying model\clend{}.
        It can also be used in a wider range of model selection tasks, even in cases
        where \cloze{2}it is hard to pinpoint the model degrees of freedom (\textit{e.g.}\
        the number of predictors in the model) or hard to estimate the error
        variance $\sigma^2$\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}


\begin{note}{itsl-614}
  \field\ The one-standard-error rule
  \field\ When choosing between models with different degrees of freedoms, the
        one-standard-error rule can be applied, where the \cloze{1} simplest model
        out of ``equally good'' models is chosen\clend{}.
        We first calculate \cloze{2}the one-standard error of the estimated test MSE for
        each model size, and then select the smallest model for which the estimated
        test error is within one standard error of the lowest point on the
        curve\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-615}
  \field\ Ridge Regression
  \field\ In linear models, ridge regression coefficients $\beta^R$ are chosen to
        minimise,
        \cloze{1}
        \[
                \text{RSS} + \lambda \sum_{j = 1}^p \beta_j^2
                    = \sum_{i = 1}^n (y_i - \beta_0 - 
                        \sum_{j = 1}^p \beta_j x_{ij})^2
                        + \lambda \sum_{j = 1}^p \beta_j^2,
                \]\clend{}
        where \cloze{1}$\lambda \geq 0$ is some tuning parameter\clend{}.
        Note that the second term, the \cloze{2} shrinkage penalyty\clend{}, is only
        \cloze{2}applied to $\beta_1, \ldots, \beta_p$, not the intercept
        $\beta_0$\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}


\begin{note}{itsl-616}
  \field\ Linear Model Least Squares
        \field\ The standard linear model least squares coefficient estimates are
        \cloze{1}scale equivariant\clend{}: multiplying $X_j$ by a constant $c$
        leads to \cloze{1}a scaling of the least squares coefficient estimates by a
        factor of $1/c$, such that $X_j \hat{\beta_j}$ will remain the same\clend{}.
  \field\ %back
  \field\ Linear Model Selection
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-617}
  \field\ Ridge Regression
  \field\ Ridge regression coeﬀicient estimates can change sub-
        stantially when \cloze{1}multiplying a given predictor by a constant (\textit{i.e.}\
        they are not scale equivariant), and therefore it is best to apply ridge
        regression after standardising the predictors\clend{}, \cloze{2}using the formula,
        \[
                \tilde{x}_{ij} = \frac{x_{ij}}{
                        \sqrt{\frac{1}{n} \sum_{i = 1}^n (x_{ij} - \bar{x}_j)^2}}.
        \]
        \clend{}

  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-618}
  \field\ Ridge Regression Advantages
  \field\ Ridge regression’s advantage over least squares is rooted in the
        \cloze{1}bias-variance trade-off\clend{}. As \cloze{1}$\lambda$ increases, the flexibility of the ridge
        regression fit decreases, leading to decreased variance but increased
        bias\clend{}.

        Ridge regression works best in scenarios \cloze{2}where the least squares solution
        has high variance, \textit{e.g.}\ if $p \approx n$\clend{}. Ridge regression can
        also perform in the \cloze{2}$p > n$ scenario, even though least squares does not
        have a unique solution\clend{}.
  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-619}
  \field\ Ridge Regression Advantages
        \field\ Ridge regression has substantial computational advantages \cloze{1}over best subset
        selection. For any fixed value of $\lambda$, ridge regression only fits a
        single model, and the model-fitting procedure can be performed quite
        quickly\clend{}.

        The computations required \cloze{2}for solving,
        \[
                \text{RSS} + \lambda \sum_{j = 1}^p \beta_j^2,
        \]

        simultaneously for all values of $\lambda$, are almost identical to those
        for fitting a model using least squares\clend{}.
  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-620}
  \field\ The Lasso
        \field\ The lasso coefficients, $\hat{\beta}_\lambda^L$, minimises the
        quantity,
        \cloze{1}
        \[
        \text{RSS} + \lambda \sum_{j = 1}^p |\beta_j| =
                \sum_{i = 1}^n \Bigl(y_i - \beta_0 - \sum_{j = 1}^p \beta_j
        x_{ij}\Bigr)^2
                    + \lambda \sum_{j = 1}^p |\beta_j|.
        \]
        \clend{}
  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-621}
  \field\ Ridge vs. Lasso
        \field\ The only difference \cloze{1}between the quantity to minimise in ridge and lasso is
        the penalty term -- ridge uses $\beta_j^2$ and lasso $|\beta_j|$\clend{}. 
        That is, ridge regression uses an \cloze{1}$l_2$ penatly term, and
        lasso uses $l_1$\clend{}.
  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-622}
  \field\ The Lasso
        \field\ The $l_1$ penalty has the effect of \cloze{1}forcing some coefficient estimates to
  be exactly equal to zero when the tuning parameter $\lambda$ is sufficiently
        large. Hence, lasso performs variable selection\clend{}.\\
        The lasso yields \cloze{2}sparse models which are often much easier to interprest
        than those produced using ridge regression\clend{}.
        
  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-623}
  \field\ Another formulation for Ridge Regression and the Lasso
  \field\ The lasso and ridge regression coefficients estimates solve the problems,
        \cloze{1}
        \[
        \min_\beta \Biggl(\sum_{i = 1}^n \Bigl(y_i - \beta_0 - \sum_{j =
        1}^p \beta_j x_{ij} \Bigr)^2 \Biggr) \quad
                \text{s.t.} \quad \sum_{j = 1}^p |\beta_j| \leq s
        \]
        \clend{}
        and,
        \cloze{1}
        \[
        \min_\beta \Biggl(\sum_{i = 1}^n \Bigl(y_i - \beta_0 - \sum_{j =
        1}^p \beta_j x_{ij} \Bigr)^2 \Biggr) \quad
                \text{s.t.} \quad \sum_{j = 1}^p \beta_j^2 \leq s,
        \]
        \clend{}
        respectively.
  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\begin{note}{itsl-624}
  \field\ The Lasso
  \field\ Consider the formulation
        \[
                \min_\beta (\text{RSS}) \quad
                \text{s.t.} \quad \sum_{j = 1}^p |\beta_j| \leq s.
        \]
        When $p = 2$, \cloze{1}this indicates that the lasso coefficient estimates
        have the smallest RSS out of all points that lie within the diamond defined
        by $|\beta_1| + |\beta_2| \leq s$\clend{}.\\
        Therefore, the lasso regression coeﬀicient estimates are given by the
        \cloze{1}first point at which an ellipse centered around $\hat{\beta}$
        contacts the constraint region\clend{}.
        As the lasso constraint has \cloze{1}corners at each of the axes, therefore
        the ellipse will often intersect the constraint region at an axis (where
        some of the coefficients will equal zero)\clend{}.
  \field\ %back
  \field\ Shinkage Methods
  \field\ %type
  \field\ %note number
\end{note}

\end{document}
