#separator:|
#html:true
itsl-501|The Validation Set Approach|"Two potential drawbacks of the validation set approach are: <ol class=""enumerate""><li><p>{{c1::The validation estimate of the test error rate can be highly variable, dependent on which observations are included in the training and test sets}}; </p></li><li><p>{{c2::Only a subset of observations (the training set) are used to train the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set}}. </p></li></ol>"||Resampling||
itsl-502|Leave-One-Out Cross-Validation|"LOOCV involves leaving a single observations out of the training set, building the model using the remaining observations and calculating the MSE on the test set observation, \(\text{MSE}_j\).<br/>The LOOCV estimate for the test MSE is the {{c1:: average of the \(n\) test error estimates,}} {{c2:: <div class=""displaymath"" id=""a0000000002"">  \[  CV_{(n)} = \frac{1}{n} \sum _{i = 1}^n \text{MSE}_i.  \]</div> }}"||Resampling||
itsl-503|Leave-One-Out Cross-Validation|"With least squares linear or polynomial regression, the cost of LOOCV is the same as a single model fit. The CV formula is, {{c1:: <div class=""displaymath"" id=""a0000000003"">  \[  \text{CV}_{(n)} = \frac{1}{n} \sum _{i = 1}^n \Bigl(\frac{y_i - \hat{y}_i}{1 - h_i}\Bigr)^2,  \]</div> }} where {{c1::\(\hat y_i\) is the \(i^\text {th}\) fitted value from the original least squares fit}}, and {{c1::\(h_i\) is the leverage statistic}}."||Resampling||
itsl-504|\(k\)-fold Cross-Validation|"\(k\)-fold CV involves randomly dividing the set of observations into \(k\) groups. For each fold \(j\), the model is built using the remaining folds, and MSE calculated on the \(j^\text {th}\) fold.<br/>The \(k\)-fold CV estimate is {{c1:: computed by averaging the MSE values}}, {{c1:: <div class=""displaymath"" id=""a0000000004"">  \[  CV_{(k)} = \frac{1}{k} \sum _{i = 1}^k \text{MSE}_i.  \]</div>}}"|||Resampling|
itsl-505|LOOCV vs. \(k\)-fold CV|LOOCV has {{c1:: lower bias}} then \(k\)-folds CV as {{c1:: more observations are included in the training set}}.<br/>However, in LOOCV, each model is trained on {{c2::an almost identical set of observations, and therefore the outputs are highly (positively) correlated}}. Since the {{c2::mean of many highly correlated quantities has higher variance then quantities that are less correlated}}, the test error estimate from LOOCV {{c2::tends to have higher variance than that resulting from \(k\)-folds CV}}.||Resampling||
itsl-506|CV on Classification Problems|"When \(Y\) is qualitative, instead of using {{c1::MSE to quantify test error, we use the number of misclassified observations}}. For example, the LOOCV classification error rate takes the form, {{c1:: <div class=""displaymath"" id=""a0000000005"">  \[  \text{CV}_{(n)} = \frac{1}{n} \sum _{i = 1}^n \text{Err}_i,  \]</div>}} where {{c1::\(\text{Err}_i = I(y_i \neq \hat y_i)\)}}."||Resampling||
itsl-507|The Bootstrap|"Given \(B\) bootstrap data sets, \(Z^{*1}, \ldots , Z^{*B}\), and \(B\) corresponding \(\alpha \) estimates, \(\hat\alpha ^{*1}, \ldots , \hat\alpha ^{*B}\), we can compute the standard error of these bootstrap estimates using the formula, {{c1:: <div class=""displaymath"" id=""a0000000006"">  \[  \text{SE}_B(\hat\alpha ) = \sqrt{\frac{1}{B - 1} \sum _{r = 1}^B \Bigl(\hat{\alpha }^{*r} - \frac{1}{B} \sum _{r' = 1}^B \hat{\alpha }^{*r'}\Bigr)^2}.  \]</div>}} This serves as an estimate of {{c2:: the standard error of \(\hat\alpha \) estimated from the original data set}}."||Resampling||
