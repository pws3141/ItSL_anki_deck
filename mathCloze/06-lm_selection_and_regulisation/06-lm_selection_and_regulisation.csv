#separator:|
#html:true
itsl-601|Best Subset Selection|"To perform best subset selection, we fit {{c1::a seperate least squares regression for each possible combination of the \(p\) predictors}}. <p>The procedure is (where \(\mathcal{M}_0\) is the null model with no predictors): </p><ol class=""enumerate""><li><p>For \(k = 1, \ldots , p\): {{c2:: </p><ol class=""enumerate""><li><p>Fit all \({p \choose k}\) models that contain exactly \(k\) predictors; </p></li><li><p>Pick the “best” model, \(\mathcal{M}_k\), such that <i class=""itshape"">e.g.</i> RSS is minimised, or \(R^2\) maximised. </p></li></ol><p> }} </p></li><li><p>{{c2::Select the single best model from \(\mathcal{M}_0, \ldots , \mathcal{M}_p\), using <i class=""itshape"">e.g.</i> the prediction error on a validation set, adjusted \(R^2\), or cross validation.}} </p></li></ol><p> </p>"||Linear Model Selection||
itsl-602|Best Subset Selection|Best subset selection suffers from {{c1::computational limitations}}. In general, there are {{c1::\(2^p\) models that involve subsets of \(p\) predictors}}.||Linear Model Selection||
itsl-603|Forward Stepwise Selection|Forward stepwise selection begins with {{c1::a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model}}. At each step {{c1::the variable that gives the greatest additional improvement to the fit is added to the model}}.||Linear Model Selection||
itsl-604|Forward Stepwise Selection|Forward stepwise selection involves fitting a total of {{c1::\(\sum _{i = 0}^{p - 1} (p - k) = 1 + \frac{p(p + 1)}{2}\)}} models.||Linear Model Selection||
itsl-605|Forward Stepwise Selection|Forward stepwise selection can be applied even in the scenario {{c1::\(n {\lt} p\), although here only the first \(n\) stepwise models, \(\mathcal{M}_0, \ldots , \mathcal{M}_{n - 1}\), can be found}}. Beyond this, {{c1::least squares does not give a unique solution}}.||Linear Model Selection||
itsl-606|Adjusting the training error|Four approaches to selecting among a set of models with different number of variables are: {{c1::\(C_p\), Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted \(R^2\)}}.||Linear Model Selection||
itsl-607|\(C_p\) estimate of test MSE|"For a fitted least squares model containing \(d\) predictors, the \(C_p\) estimate of test MSE is given by, {{c1:: <div class=""displaymath"" id=""a0000000002"">  \[  C_p = \frac{1}{n} (RSS + 2d \hat{\sigma }^2),  \]</div> }} where {{c1::\(\hat{\sigma }^2\) is an estimate of the variance of the error \(\varepsilon \) associated with each response measurement}}. Typically, {{c2::\(\hat{\sigma }^2\) is estimated using the full model containing all predictors}}."||Linear Model Selection||
itsl-608|\(C_p\) estimate of test MSE|Essentially, the \(C_p\) statistic {{c1::adds a penalty of \(2d\hat{\sigma }^2\) to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error}}.||Linear Model Selection||
itsl-609|Akaike Information Criterion|"For a standard multiple regression model, least squares and maximum likelihood are the same. In this case AIC is given by {{c1:: <div class=""displaymath"" id=""a0000000003"">  \[  \text{AIC} \propto \frac{1}{n} (RSS + 2d \hat{\sigma }^2).  \]</div> }} Hence, {{c1:: for least squares models, \(C_p\) and AIC are proportional to each other}}."||Linear Model Selection||
itsl-610|Bayesian Information Criterion|"For a least squares model with \(d\) predictors, the BIC is given by {{c1:: <div class=""displaymath"" id=""a0000000004"">  \[  \text{BIC} \propto \frac{1}{n} (RSS + \log (n) d \hat{\sigma }^2).  \]</div> }}"||Linear Model Selection||
itsl-611|Adjusted-\(R^2\)|"For a least squares model with \(d\) predictors, the adjusted \(R^2\) statistic is given by {{c1:: <div class=""displaymath"" id=""a0000000005"">  \[  \text{Adjusted } R^2 = 1 - \frac{\text{RSS} / (n - d- 1)}{\text{TSS} / (n - 1)}.  \]</div> }} A {{c2:: large value }} of adjusted \(R^2\) indicates a model with {{c2:: small test error }}."||Linear Model Selection||
itsl-612|Adjusted-\(R^2\)|The intuition behind the adjusted \(R^2\) is that once all of the correct variables have been included in the model, adding {{c1::additional noise variables will lead to only a very small decrease in RSS}}. Since adding {{c1::noise variables leads to an increase in \(d\), such variables will lead to an increase in \(RSS/(n - d - 1)\), and consequently a decrease in the adjusted \(R^2\)}}.||Linear Model Selection||
itsl-613|Validation and cross-validation|"Using validation and cross-validation procedures to estimate test error has an advantage relative to AIC, BIC, \(C_p\), and adjusted \(R^2\), in that {{c1::it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model}}. It can also be used in a wider range of model selection tasks, even in cases where {{c2::it is hard to pinpoint the model degrees of freedom (<i class=""itshape"">e.g.</i> the number of predictors in the model) or hard to estimate the error variance \(\sigma ^2\)}}."||Linear Model Selection||
itsl-614|The one-standard-error rule|When choosing between models with different degrees of freedoms, the one-standard-error rule can be applied, where the {{c1:: simplest model out of “equally good” models is chosen}}. We first calculate {{c2::the one-standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve}}.||Linear Model Selection||
itsl-615|Ridge Regression|"In linear models, ridge regression coefficients \(\beta ^R\) are chosen to minimise, {{c1:: <div class=""displaymath"" id=""a0000000006"">  \[  \text{RSS} + \lambda \sum _{j = 1}^p \beta _j^2 = \sum _{i = 1}^n (y_i - \beta _0 - \sum _{j = 1}^p \beta _j x_{ij})^2 + \lambda \sum _{j = 1}^p \beta _j^2,  \]</div>}} where {{c1::\(\lambda \geq 0\) is some tuning parameter}}. Note that the second term, the {{c2:: shrinkage penalyty}}, is only {{c2::applied to \(\beta _1, \ldots , \beta _p\), not the intercept \(\beta _0\)}}."||Linear Model Selection||
itsl-616|Linear Model Least Squares|The standard linear model least squares coefficient estimates are {{c1::scale equivariant}}: multiplying \(X_j\) by a constant \(c\) leads to {{c1::a scaling of the least squares coefficient estimates by a factor of \(1/c\), such that \(X_j \hat{\beta _j}\) will remain the same}}.||Linear Model Selection||
itsl-617|Ridge Regression|"Ridge regression coeﬀicient estimates can change sub- stantially when {{c1::multiplying a given predictor by a constant (<i class=""itshape"">i.e.</i> they are not scale equivariant), and therefore it is best to apply ridge regression after standardising the predictors}}, {{c2::using the formula, <div class=""displaymath"" id=""a0000000007"">  \[  \tilde{x}_{ij} = \frac{x_{ij} }{ \sqrt{\frac{1}{n} \sum _{i = 1}^n (x_{ij} - \bar{x}_j)^2} }.  \]</div><p> }} </p><p> </p>"||Linear Model Selection||
itsl-618|Ridge Regression Advantages|"Ridge regression’s advantage over least squares is rooted in the {{c1::bias-variance trade-off}}. As {{c1::\(\lambda \) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias}}. <p>Ridge regression works best in scenarios {{c2::where the least squares solution has high variance, <i class=""itshape"">e.g.</i> if \(p \approx n\)}}. Ridge regression can also perform in the {{c2::\(p {\gt} n\) scenario, even though least squares does not have a unique solution}}.  </p>"||Linear Model Selection||
itsl-619|Ridge Regression Advantages|"Ridge regression has substantial computational advantages {{c1::over best subset selection. For any fixed value of \(\lambda \), ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly}}. <p>The computations required {{c2::for solving, </p><div class=""displaymath"" id=""a0000000008"">  \[  \text{RSS} + \lambda \sum _{j = 1}^p \beta _j^2,  \]</div><p>simultaneously for all values of \(\lambda \), are almost identical to those for fitting a model using least squares}}.  </p>"||Linear Model Selection||
