#separator:|
#html:true
itsl-601|Best Subset Selection|"To perform best subset selection, we fit {{c1::a seperate least squares regression for each possible combination of the \(p\) predictors}}. <p>The procedure is (where \(\mathcal{M}_0\) is the null model with no predictors): </p><ol class=""enumerate""><li><p>For \(j = 1, \ldots , p\): {{c2:: </p><ol class=""enumerate""><li><p>Fit all \({p \choose k}\) models that contain exactly \(k\) predictors; </p></li><li><p>Pick the “best” model, \(\mathcal{M}_k\), such that <i class=""itshape"">e.g.</i> RSS is minimised, or \(R^2\) maximised. </p></li></ol><p> }} </p></li><li><p>{{c2::Select the single best model from \(\mathcal{M}_0, \ldots , \mathcal{M}_p\), using <i class=""itshape"">e.g.</i> the prediction error on a validation set, adjusted \(R^2\), or cross validation.}} </p></li></ol><p> </p>"||Linear Model Selection||
itsl-602|Best Subset Selection|Best subset selection suffers from {{c1::computational limitations}}. In general, there are {{c1::\(2^p\) models that involve subsets of \(p\) predictors}}.||Linear Model Selection||
itsl-603|Forward Stepwise Selection|Forward stepwise selection begins with {{c1::a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model}}. At each step {{c1::the variable that gives the greatest additional improvement to the fit is added to the model}}.||Linear Model Selection||
itsl-604|Forward Stepwise Selection|Forward stepwise selection involves fitting a total of {{c1::\(\sum _{i = 0}^{p - 1} (p - k) = 1 + \frac{p(p + 1)}{2}\)}} models.||Linear Model Selection||
itsl-605|Forward Stepwise Selection|Forward stepwise selection can be applied even in the scenario {{c1::\(n {\lt} p\), although here only the first \(n\) stepwise models, \(\mathcal{M}_0, \ldots , \mathcal{M}_{n - 1}\), can be found}}. Beyond this, {{c1::least squares does not give a unique solution}}.||Linear Model Selection||
itsl-606|Adjusting the training error|Four approaches to selecting among a set of models with different number of variables are: {{c1::\(C_p\), Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted \(R^2\)}}.||Linear Model Selection||
itsl-607|\(C_p\) estimate of test MSE|"For a fitted least squares model containing \(d\) predictors, the \(C_p\) estimate of test MSE is given by, {{c1:: <div class=""displaymath"" id=""a0000000002"">  \[  C_p = \frac{1}{n} (RSS + 2d \hat{\sigma }^2),  \]</div> }} where {{c1::\(\hat{\sigma }^2\) is an estimate of the variance of the error \(\varepsilon \) associated with each response measurement}}. Typically, {{c2::\(\hat{\sigma }^2\) is estimated using the full model containing all predictors}}."||Linear Model Selection||
itsl-608|\(C_p\) estimate of test MSE|Essentially, the \(C_p\) statistic {{c1::adds a penalty of \(2d\hat{\sigma }^2\) to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error}}.||Linear Model Selection||
itsl-609|Akaike Information Criterion|"For a standard multiple regression model, least squares and maximum likelihood are the same. In this case AIC is given by {{c1:: <div class=""displaymath"" id=""a0000000003"">  \[  \text{AIC} \propto \frac{1}{n} (RSS + 2d \hat{\sigma }^2).  \]</div> }} Hence, {{c1:: for least squares models, \(C_p\) and AIC are proportional to each other}}."||Linear Model Selection||
itsl-610|Bayesian Information Criterion|"For a least squares model with \(d\) predictors, the BIC is given by {{c1:: <div class=""displaymath"" id=""a0000000004"">  \[  \text{BIC} \propto \frac{1}{n} (RSS + \log (n) d \hat{\sigma }^2).  \]</div> }}"||Linear Model Selection||
itsl-611|Adjusted-\(R^2\)|"For a least squares model with \(d\) predictors, the adjusted \(R^2\) statistic is given by {{c1:: <div class=""displaymath"" id=""a0000000005"">  \[  \text{Adjusted } R^2 = 1 - \frac{\text{RSS} / (n - d- 1)}{\text{TSS} / (n - 1)}.  \]</div> }} A {{c2:: large value }} of adjusted \(R^2\) indicates a model with {{c2:: small test error }}."||Linear Model Selection||
itsl-612|Adjusted-\(R^2\)|The intuition behind the adjusted \(R^2\) is that once all of the correct variables have been included in the model, adding {{c1::additional noise variables will lead to only a very small decrease in RSS}}. Since adding {{c1::noise variables leads to an increase in \(d\), such variables will lead to an increase in \(RSS/(n - d - 1)\), and consequently a decrease in the adjusted \(R^2\)}}.||Linear Model Selection||
itsl-613|Validation and cross-validation|"Using validation and cross-validation procedures to estimate test error has an advantage relative to AIC, BIC, \(C_p\), and adjusted \(R^2\), in that {{c1::it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model}}. It can also be used in a wider range of model selection tasks, even in cases where {{c2::it is hard to pinpoint the model degrees of freedom (<i class=""itshape"">e.g.</i> the number of predictors in the model) or hard to estimate the error variance \(\sigma ^2\)}}."||Linear Model Selection||
itsl-614|The one-standard-error rule|When choosing between models with different degrees of freedoms, the one-standard-error rule can be applied, where the {{c1:: simplest model out of “equally good” models is chosen}}. We first calculate {{c2::the one-standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve}}.||Linear Model Selection||
itsl-615|Ridge Regression|"In linear models, ridge regression coefficients \(\beta ^R\) are chosen to minimise, {{c1:: <div class=""displaymath"" id=""a0000000006"">  \[  \text{RSS} + \lambda \sum _{j = 1}^p \beta _j^2 = \sum _{i = 1}^n (y_i - \beta _0 - \sum _{j = 1}^p \beta _j x_{ij})^2 + \lambda \sum _{j = 1}^p \beta _j^2,  \]</div>}} where {{c1::\(\lambda \geq 0\) is some tuning parameter}}."||Linear Model Selection||
