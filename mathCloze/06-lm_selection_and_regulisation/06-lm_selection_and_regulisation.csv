#separator:|
#html:true
itsl-601|Best Subset Selection|"To perform best subset selection, we fit {{c1::a seperate least squares regression for each possible combination of the \(p\) predictors}}. <p>The procedure is (where \(\mathcal{M}_0\) is the null model with no predictors): </p><ol class=""enumerate""><li><p>For \(k = 1, \ldots , p\): {{c2:: </p><ol class=""enumerate""><li><p>Fit all \({p \choose k}\) models that contain exactly \(k\) predictors; </p></li><li><p>Pick the “best” model, \(\mathcal{M}_k\), such that <i class=""itshape"">e.g.</i> RSS is minimised, or \(R^2\) maximised. </p></li></ol><p> }} </p></li><li><p>{{c2::Select the single best model from \(\mathcal{M}_0, \ldots , \mathcal{M}_p\), using <i class=""itshape"">e.g.</i> the prediction error on a validation set, adjusted \(R^2\), or cross validation.}} </p></li></ol><p> </p>"||Linear Model Selection||
itsl-602|Best Subset Selection|Best subset selection suffers from {{c1::computational limitations}}. In general, there are {{c1::\(2^p\) models that involve subsets of \(p\) predictors}}.||Linear Model Selection||
itsl-603|Forward Stepwise Selection|Forward stepwise selection begins with {{c1::a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model}}. At each step {{c1::the variable that gives the greatest additional improvement to the fit is added to the model}}.||Linear Model Selection||
itsl-604|Forward Stepwise Selection|Forward stepwise selection involves fitting a total of {{c1::\(\sum _{i = 0}^{p - 1} (p - k) = 1 + \frac{p(p + 1)}{2}\)}} models.||Linear Model Selection||
itsl-605|Forward Stepwise Selection|Forward stepwise selection can be applied even in the scenario {{c1::\(n {\lt} p\), although here only the first \(n\) stepwise models, \(\mathcal{M}_0, \ldots , \mathcal{M}_{n - 1}\), can be found}}. Beyond this, {{c1::least squares does not give a unique solution}}.||Linear Model Selection||
itsl-606|Adjusting the training error|Four approaches to selecting among a set of models with different number of variables are: {{c1::\(C_p\), Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted \(R^2\)}}.||Linear Model Selection||
itsl-607|\(C_p\) estimate of test MSE|"For a fitted least squares model containing \(d\) predictors, the \(C_p\) estimate of test MSE is given by, {{c1:: <div class=""displaymath"" id=""a0000000002"">  \[  C_p = \frac{1}{n} (RSS + 2d \hat{\sigma }^2),  \]</div> }} where {{c1::\(\hat{\sigma }^2\) is an estimate of the variance of the error \(\varepsilon \) associated with each response measurement}}. Typically, {{c2::\(\hat{\sigma }^2\) is estimated using the full model containing all predictors}}."||Linear Model Selection||
itsl-608|\(C_p\) estimate of test MSE|Essentially, the \(C_p\) statistic {{c1::adds a penalty of \(2d\hat{\sigma }^2\) to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error}}.||Linear Model Selection||
itsl-609|Akaike Information Criterion|"For a standard multiple regression model, least squares and maximum likelihood are the same. In this case AIC is given by {{c1:: <div class=""displaymath"" id=""a0000000003"">  \[  \text{AIC} \propto \frac{1}{n} (RSS + 2d \hat{\sigma }^2).  \]</div> }} Hence, {{c1:: for least squares models, \(C_p\) and AIC are proportional to each other}}."||Linear Model Selection||
itsl-610|Bayesian Information Criterion|"For a least squares model with \(d\) predictors, the BIC is given by {{c1:: <div class=""displaymath"" id=""a0000000004"">  \[  \text{BIC} \propto \frac{1}{n} (\text{RSS} + \log (n) d \hat{\sigma }^2).  \]</div> }}"||Linear Model Selection||
itsl-611|Adjusted-\(R^2\)|"For a least squares model with \(d\) predictors, the adjusted \(R^2\) statistic is given by {{c1:: <div class=""displaymath"" id=""a0000000005"">  \[  \text{Adjusted } R^2 = 1 - \frac{\text{RSS} / (n - d- 1)}{\text{TSS} / (n - 1)}.  \]</div> }} A {{c2:: large value }} of adjusted \(R^2\) indicates a model with {{c2:: small test error }}."||Linear Model Selection||
itsl-612|Adjusted-\(R^2\)|The intuition behind the adjusted \(R^2\) is that once all of the correct variables have been included in the model, adding {{c1::additional noise variables will lead to only a very small decrease in RSS}}. Since adding {{c1::noise variables leads to an increase in \(d\), such variables will lead to an increase in \(RSS/(n - d - 1)\), and consequently a decrease in the adjusted \(R^2\)}}.||Linear Model Selection||
itsl-613|Validation and cross-validation|"Using validation and cross-validation procedures to estimate test error has an advantage relative to AIC, BIC, \(C_p\), and adjusted \(R^2\), in that {{c1::it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model}}. It can also be used in a wider range of model selection tasks, even in cases where {{c2::it is hard to pinpoint the model degrees of freedom (<i class=""itshape"">e.g.</i> the number of predictors in the model) or hard to estimate the error variance \(\sigma ^2\)}}."||Linear Model Selection||
itsl-614|The one-standard-error rule|When choosing between models with different degrees of freedoms, the one-standard-error rule can be applied, where the {{c1:: simplest model out of “equally good” models is chosen}}. We first calculate {{c2::the one-standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve}}.||Linear Model Selection||
itsl-615|Ridge Regression|"In linear models, ridge regression coefficients \(\beta ^R\) are chosen to minimise, {{c1:: <div class=""displaymath"" id=""a0000000006"">  \[  \text{RSS} + \lambda \sum _{j = 1}^p \beta _j^2 = \sum _{i = 1}^n (y_i - \beta _0 - \sum _{j = 1}^p \beta _j x_{ij})^2 + \lambda \sum _{j = 1}^p \beta _j^2,  \]</div>}} where {{c1::\(\lambda \geq 0\) is some tuning parameter}}. Note that the second term, the {{c2:: shrinkage penalyty}}, is only {{c2::applied to \(\beta _1, \ldots , \beta _p\), not the intercept \(\beta _0\)}}."||Linear Model Selection||
itsl-616|Linear Model Least Squares|The standard linear model least squares coefficient estimates are {{c1::scale equivariant}}: multiplying \(X_j\) by a constant \(c\) leads to {{c1::a scaling of the least squares coefficient estimates by a factor of \(1/c\), such that \(X_j \hat{\beta _j}\) will remain the same}}.||Linear Model Selection||
itsl-617|Ridge Regression|"Ridge regression coeﬀicient estimates can change sub- stantially when {{c1::multiplying a given predictor by a constant (<i class=""itshape"">i.e.</i> they are not scale equivariant), and therefore it is best to apply ridge regression after standardising the predictors}}, {{c2::using the formula, <div class=""displaymath"" id=""a0000000007"">  \[  \tilde{x}_{ij} = \frac{x_{ij} }{ \sqrt{\frac{1}{n} \sum _{i = 1}^n (x_{ij} - \bar{x}_j)^2} }.  \]</div><p> }} </p><p> </p>"||Shinkage Methods||
itsl-618|Ridge Regression Advantages|"Ridge regression’s advantage over least squares is rooted in the {{c1::bias-variance trade-off}}. As {{c1::\(\lambda \) increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias}}. <p>Ridge regression works best in scenarios {{c2::where the least squares solution has high variance, <i class=""itshape"">e.g.</i> if \(p \approx n\)}}. Ridge regression can also perform in the {{c2::\(p {\gt} n\) scenario, even though least squares does not have a unique solution}}.  </p>"||Shinkage Methods||
itsl-619|Ridge Regression Advantages|"Ridge regression has substantial computational advantages {{c1::over best subset selection. For any fixed value of \(\lambda \), ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly}}. <p>The computations required {{c2::for solving, </p><div class=""displaymath"" id=""a0000000008"">  \[  \text{RSS} + \lambda \sum _{j = 1}^p \beta _j^2,  \]</div><p>simultaneously for all values of \(\lambda \), are almost identical to those for fitting a model using least squares}}.  </p>"||Shinkage Methods||
itsl-620|The Lasso|"The lasso coefficients, \(\hat{\beta }_\lambda ^L\), minimises the quantity, {{c1:: <div class=""displaymath"" id=""a0000000009"">  \[  \text{RSS} + \lambda \sum _{j = 1}^p |\beta _j| = \sum _{i = 1}^n \Bigl(y_i - \beta _0 - \sum _{j = 1}^p \beta _j x_{ij}\Bigr)^2 + \lambda \sum _{j = 1}^p |\beta _j|.  \]</div> }}"||Shinkage Methods||
itsl-621|Ridge vs. Lasso|"The only difference {{c1::between the quantity to minimise in ridge and lasso is the penalty term – ridge uses \(\beta _j^2\) and lasso \(|\beta _j|\)}}. That is, ridge regression uses an {{c1::\(l_2\) penatly term, and lasso uses \(l_1\)}}."||Shinkage Methods||
itsl-622|The Lasso|The \(l_1\) penalty has the effect of {{c1::forcing some coefficient estimates to be exactly equal to zero when the tuning parameter \(\lambda \) is sufficiently large. Hence, lasso performs variable selection}}.<br/>The lasso yields {{c2::sparse models which are often much easier to interprest than those produced using ridge regression}}. <p> </p>||Shinkage Methods||
itsl-623|Another formulation for Ridge Regression and the Lasso|"The lasso and ridge regression coefficients estimates solve the problems, {{c1:: <div class=""displaymath"" id=""a0000000010"">  \[  \min _\beta \Biggl(\sum _{i = 1}^n \Bigl(y_i - \beta _0 - \sum _{j = 1}^p \beta _j x_{ij} \Bigr)^2 \Biggr) \quad \text{s.t.} \quad \sum _{j = 1}^p |\beta _j| \leq s  \]</div> }} and, {{c1:: <div class=""displaymath"" id=""a0000000011"">  \[  \min _\beta \Biggl(\sum _{i = 1}^n \Bigl(y_i - \beta _0 - \sum _{j = 1}^p \beta _j x_{ij} \Bigr)^2 \Biggr) \quad \text{s.t.} \quad \sum _{j = 1}^p \beta _j^2 \leq s,  \]</div> }} respectively."||Shinkage Methods||
itsl-624|The Lasso|"Consider the formulation <div class=""displaymath"" id=""a0000000012"">  \[  \min _\beta (\text{RSS}) \quad \text{s.t.} \quad \sum _{j = 1}^p |\beta _j| \leq s.  \]</div> When \(p = 2\), {{c1::this indicates that the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by \(|\beta _1| + |\beta _2| \leq s\)}}.<br/>Therefore, the lasso regression coeﬀicient estimates are given by the {{c1::first point at which an ellipse centered around \(\hat{\beta }\) contacts the constraint region}}. As the lasso constraint has {{c1::corners at each of the axes, therefore the ellipse will often intersect the constraint region at an axis (where some of the coefficients will equal zero)}}."||Shinkage Methods||
